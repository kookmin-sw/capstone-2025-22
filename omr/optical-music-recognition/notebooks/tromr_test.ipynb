{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xml -> annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def load_xml_data(file_path: str):\n",
    "    \"\"\"\n",
    "    xml data 불러오기\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(file_path)  # XML 파일을 파싱\n",
    "        root = tree.getroot()\n",
    "        return root\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"XML 파일을 파싱하는 동안 오류가 발생했습니다: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH=f\"../data\"\n",
    "MODEL_PATH=f\"../model\"\n",
    "IMAGE_PATH=f\"../images/\"\n",
    "\n",
    "DATA_FEATURE_PATH=f\"{DATA_PATH}/processed-feature\"\n",
    "DATA_RAW_PATH=f\"{DATA_PATH}/raw\"\n",
    "DATA_TEST_PATH=f\"{DATA_PATH}/test\"\n",
    "\n",
    "OSMD=\"osmd-dataset-v1.0.0\"\n",
    "\n",
    "\n",
    "xml_path = f'{DATA_RAW_PATH}/{OSMD}/Rock-ver/Rock-ver.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\"\"\"\n",
    "- clef-purcussion 추출\n",
    "- time-signature 추출\n",
    "- multiple pitch 추출\n",
    "    <chord/> <-  얘 있으면 동시에 친 거임\n",
    "    <unpitched>\n",
    "        <display-step>A</display-step>\n",
    "        <display-octave>5</display-octave>\n",
    "    </unpitched>\n",
    "- 쉼표 추출\n",
    "    <note>\n",
    "        <rest/>\n",
    "        <duration>48</duration>\n",
    "        <type>quarter</type>\n",
    "    </note>\n",
    "\n",
    "output : [\"clef-F4+keySignature-CM+note-E3_eighth.|note-C4_eighth.+note-E3_sixteenth|...\", \"...\"]\n",
    "\"\"\"\n",
    "\n",
    "# 0. stave 마다 새로운 string 생성: print new-system 일 때마다 \n",
    "# 1. clef-purcussion 삽입: xml에선 맨 처음에만 나오니까 매번 삽입\n",
    "# 2. time-signature 있으면 삽입\n",
    "# 3. note 삽입: pitch, duration\n",
    "#    rest 삽입: duration\n",
    "# 4. if 동시에 나온 note일 시, | 으로 구분\n",
    "# 5. else + 로 연결\n",
    "\n",
    "# MusicXML 파일을 파싱하여 ElementTree 객체 생성\n",
    "tree = ET.parse(xml_path)\n",
    "root = tree.getroot()\n",
    "divisions_element = root.find(\".//divisions\")\n",
    "if divisions_element is not None:\n",
    "    divisions_value = int(divisions_element.text)\n",
    "division = divisions_value\n",
    "# 각 stave에 대한 문자열을 저장할 리스트\n",
    "stave_strings = []\n",
    "\n",
    "\n",
    "# 각 measure에 대한 처리 함수\n",
    "def process_measure(measure):\n",
    "    stave_string = \"\"\n",
    "\n",
    "    for element in measure:\n",
    "        if element.tag == 'attributes':\n",
    "            # # clef 처리\n",
    "            # clef = element.find('clef')\n",
    "            # if clef is not None:\n",
    "            #     stave_string += f\"clef-{clef.find('sign').text}\"\n",
    "\n",
    "            # time signature 처리\n",
    "            time = element.find('time')\n",
    "            if time is not None:\n",
    "                stave_string += f\"timeSignature-{time.find('beats').text}/{time.find('beat-type').text}+\"\n",
    "\n",
    "        elif element.tag == 'note':\n",
    "            if element.find('rest') is not None:\n",
    "                stave_string += \"rest\"\n",
    "            elif element.find('unpitched') is not None:\n",
    "                # note 정보 처리\n",
    "                pitch = element.find('unpitched')\n",
    "                stave_string += f\"note-{pitch.find('display-step').text}{pitch.find('display-octave').text}\"\n",
    "            \n",
    "            rhythm = element.find('type').text\n",
    "            # 16th -> sixteenth, 32nd → thirty_second\n",
    "            if rhythm==\"16th\":\n",
    "                rhythm=\"sixteenth\"\n",
    "            elif rhythm==\"32nd\":\n",
    "                rhythm=\"thirty_second\"\n",
    "            stave_string += f\"_{rhythm}+\"\n",
    "\n",
    "            if element.find(\"grace\") is None:\n",
    "                # 0.25 0.375 0.5 0.75 1.0 1.5 2.0 3.0 4.0 중에서\n",
    "                # 0.375 0.75 1.5 3 이면 뒤에 . 붙이기\n",
    "                duration_value = element.find('duration').text\n",
    "                duration = float(duration_value) / float(division)\n",
    "                d_ = '.' if duration in [0.375, 0.75, 1.5, 3]  else \"\"\n",
    "                stave_string = stave_string[:-1]+f\"{d_}+\"\n",
    "\n",
    "            # chord 여부에 따라 | 또는 + 추가\n",
    "            if element.find('chord') is not None:\n",
    "                # clef-percussion+note-F4_quarter+(<- 이걸 '|'로) note-A5_quarter+\n",
    "                # note-G5_eighth+note-C5_eighth+note-G5_eighth+ 에서 끝의 + 빼고 reverse 한 후 \n",
    "                # replace('+', '|', 1) 1개의 +만 |로 replace 한 후\n",
    "                # 다시 reverse 후, 뒤에 + 붙여서 완성!\n",
    "                components = stave_string[:-1][::-1]\n",
    "                components = components.replace('+', '|', 1)\n",
    "                stave_string = components[::-1]+'+'\n",
    "                \n",
    "    return stave_string\n",
    "\n",
    "# # measure 태그를 가진 모든 element에 대해 처리\n",
    "# stave_tmp=\"clef-percussion+\"\n",
    "# for measure in root.findall('.//measure'):\n",
    "#     # print 태그 확인하여 new-system이 있는 경우에는 새로운 문자열로 시작\n",
    "#     if measure.find('print') is not None and measure.find('print').get('new-system') == 'yes':\n",
    "#         stave_strings.append(stave_tmp[:-1])\n",
    "#         stave_tmp=\"clef-percussion+\"\n",
    "\n",
    "#     stave_tmp+=process_measure(measure)+\"barline+\"\n",
    "\n",
    "print(stave_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from timm.models.vision_transformer import VisionTransformer\n",
    "from timm.models.vision_transformer_hybrid import HybridEmbed\n",
    "from timm.models.resnetv2 import ResNetV2\n",
    "from timm.models.layers import StdConv2dSame\n",
    "from einops import repeat\n",
    "\n",
    "\n",
    "class CustomVisionTransformer(VisionTransformer):\n",
    "    def __init__(self, img_size, patch_size=16, *args, **kwargs):\n",
    "        super(CustomVisionTransformer, self).__init__(\n",
    "            img_size=img_size, patch_size=patch_size, *args, **kwargs\n",
    "        )\n",
    "        self.height, self.width = img_size\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B, c, h, w = x.shape\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        h, w = h // self.patch_size, w // self.patch_size\n",
    "        pos_emb_ind = repeat(\n",
    "            torch.arange(h) * (self.width // self.patch_size - w), \"h -> (h w)\", w=w\n",
    "        ) + torch.arange(h * w)\n",
    "        pos_emb_ind = torch.cat((torch.zeros(1), pos_emb_ind + 1), dim=0).long()\n",
    "        x += self.pos_embed[:, pos_emb_ind]\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_encoder(args):\n",
    "    backbone_layers = list(args.backbone_layers)\n",
    "    backbone = ResNetV2(\n",
    "        layers=backbone_layers,\n",
    "        num_classes=0,\n",
    "        global_pool=\"\",\n",
    "        in_chans=args.channels,\n",
    "        preact=False,\n",
    "        stem_type=\"same\",\n",
    "        conv_layer=StdConv2dSame,\n",
    "    )\n",
    "    min_patch_size = 2 ** (len(backbone_layers) + 1)\n",
    "\n",
    "    def embed_layer(**x):\n",
    "        ps = x.pop(\"patch_size\", min_patch_size)\n",
    "        assert ps % min_patch_size == 0 and ps >= min_patch_size, (\n",
    "            \"patch_size needs to be multiple of %i with current backbone configuration\"\n",
    "            % min_patch_size\n",
    "        )\n",
    "        return HybridEmbed(**x, patch_size=ps // min_patch_size, backbone=backbone)\n",
    "\n",
    "    encoder = CustomVisionTransformer(\n",
    "        img_size=(args.max_height, args.max_width),\n",
    "        patch_size=args.patch_size,\n",
    "        in_chans=args.channels,\n",
    "        num_classes=0,\n",
    "        embed_dim=args.encoder_dim,\n",
    "        depth=args.encoder_depth,\n",
    "        num_heads=args.encoder_heads,\n",
    "        embed_layer=embed_layer,\n",
    "        global_pool=\"\",\n",
    "    )\n",
    "    return encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from x_transformers.x_transformers import (\n",
    "    AttentionLayers,\n",
    "    TokenEmbedding,\n",
    "    AbsolutePositionalEmbedding,\n",
    "    Decoder,\n",
    ")\n",
    "\n",
    "\n",
    "class ScoreTransformerWrapper(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_note_tokens,\n",
    "        num_rhythm_tokens,\n",
    "        num_pitch_tokens,\n",
    "        # num_lift_tokens,\n",
    "        max_seq_len,\n",
    "        attn_layers,\n",
    "        emb_dim,\n",
    "        l2norm_embed=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert isinstance(\n",
    "            attn_layers, AttentionLayers\n",
    "        ), \"attention layers must be one of Encoder or Decoder\"\n",
    "\n",
    "        dim = attn_layers.dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.l2norm_embed = l2norm_embed\n",
    "        # self.lift_emb = TokenEmbedding(emb_dim, num_lift_tokens, l2norm_embed = l2norm_embed)\n",
    "        self.pitch_emb = TokenEmbedding(\n",
    "            emb_dim, num_pitch_tokens, l2norm_embed=l2norm_embed\n",
    "        )\n",
    "        self.rhythm_emb = TokenEmbedding(\n",
    "            emb_dim, num_rhythm_tokens, l2norm_embed=l2norm_embed\n",
    "        )\n",
    "        self.pos_emb = AbsolutePositionalEmbedding(\n",
    "            emb_dim, max_seq_len, l2norm_embed=l2norm_embed\n",
    "        )\n",
    "\n",
    "        self.project_emb = nn.Linear(emb_dim, dim) if emb_dim != dim else nn.Identity()\n",
    "        self.attn_layers = attn_layers\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.init_()\n",
    "\n",
    "        # self.to_logits_lift = nn.Linear(dim, num_lift_tokens)\n",
    "        self.to_logits_pitch = nn.Linear(dim, num_pitch_tokens)\n",
    "        self.to_logits_rhythm = nn.Linear(dim, num_rhythm_tokens)\n",
    "        self.to_logits_note = nn.Linear(dim, num_note_tokens)\n",
    "\n",
    "    def init_(self):\n",
    "        if self.l2norm_embed:\n",
    "            # nn.init.normal_(self.lift_emb.emb.weight, std=1e-5)\n",
    "            nn.init.normal_(self.pitch_emb.emb.weight, std=1e-5)\n",
    "            nn.init.normal_(self.rhythm_emb.emb.weight, std=1e-5)\n",
    "            nn.init.normal_(self.pos_emb.emb.weight, std=1e-5)\n",
    "            return\n",
    "\n",
    "        # nn.init.kaiming_normal_(self.lift_emb.emb.weight)\n",
    "        nn.init.kaiming_normal_(self.pitch_emb.emb.weight)\n",
    "        nn.init.kaiming_normal_(self.rhythm_emb.emb.weight)\n",
    "\n",
    "    def forward(self, rhythms, pitchs, mask=None, return_hiddens=True, **kwargs):\n",
    "        x = (\n",
    "            self.rhythm_emb(rhythms)\n",
    "            + self.pitch_emb(pitchs)\n",
    "            # + self.lift_emb(lifts)\n",
    "            + self.pos_emb(rhythms)\n",
    "        )\n",
    "        x = self.project_emb(x)\n",
    "        x, hiddens = self.attn_layers(\n",
    "            x, mask=mask, return_hiddens=return_hiddens, **kwargs\n",
    "        )\n",
    "        select_hiddens = hiddens[0][3]\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # out_lifts = self.to_logits_lift(x)\n",
    "        out_pitchs = self.to_logits_pitch(x)\n",
    "        out_rhythms = self.to_logits_rhythm(x)\n",
    "        out_notes = self.to_logits_note(x)\n",
    "        # return out_rhythms, out_pitchs, out_lifts, out_notes, x\n",
    "        return out_rhythms, out_pitchs, out_notes, x\n",
    "\n",
    "\n",
    "def top_k(logits, thres=0.9):\n",
    "    k = ceil((1 - thres) * logits.shape[-1])\n",
    "    val, ind = torch.topk(logits, k)\n",
    "    probs = torch.full_like(logits, float(\"-inf\"))\n",
    "    probs.scatter_(1, ind, val)\n",
    "    return probs\n",
    "\n",
    "\n",
    "class ScoreDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, transoformer, noteindexes, num_rhythmtoken, ignore_index=-100, pad_value=0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pad_value = pad_value\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "        self.net = transoformer\n",
    "        self.max_seq_len = transoformer.max_seq_len\n",
    "\n",
    "        note_mask = torch.zeros(num_rhythmtoken)\n",
    "        note_mask[noteindexes] = 1\n",
    "        self.note_mask = nn.Parameter(note_mask)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        start_tokens,\n",
    "        nonote_tokens,\n",
    "        seq_len,\n",
    "        eos_token=None,\n",
    "        temperature=1.0,\n",
    "        filter_thres=0.9,\n",
    "        min_p_pow=2.0,\n",
    "        min_p_ratio=0.02,\n",
    "        **kwargs\n",
    "    ):\n",
    "        device = start_tokens.device\n",
    "        was_training = self.net.training\n",
    "        num_dims = len(start_tokens.shape)\n",
    "\n",
    "        if num_dims == 1:\n",
    "            start_tokens = start_tokens[None, :]\n",
    "\n",
    "        b, t = start_tokens.shape\n",
    "\n",
    "        self.net.eval()\n",
    "        out_rhythm = start_tokens\n",
    "        out_pitch = nonote_tokens\n",
    "        out_lift = nonote_tokens\n",
    "        mask = kwargs.pop(\"mask\", None)\n",
    "\n",
    "        if mask is None:\n",
    "            mask = torch.full_like(\n",
    "                out_rhythm, True, dtype=torch.bool, device=out_rhythm.device\n",
    "            )\n",
    "\n",
    "        for _ in range(seq_len):\n",
    "            mask = mask[:, -self.max_seq_len :]\n",
    "            # x_lift = out_lift[:, -self.max_seq_len :]\n",
    "            x_pitch = out_pitch[:, -self.max_seq_len :]\n",
    "            x_rhymthm = out_rhythm[:, -self.max_seq_len :]\n",
    "\n",
    "            rhythmsp, pitchsp, notesp, _ = self.net(\n",
    "                x_rhymthm, x_pitch, mask=mask, **kwargs\n",
    "            )\n",
    "\n",
    "            # filtered_lift_logits = top_k(liftsp[:, -1, :], thres=filter_thres)\n",
    "            filtered_pitch_logits = top_k(pitchsp[:, -1, :], thres=filter_thres)\n",
    "            filtered_rhythm_logits = top_k(rhythmsp[:, -1, :], thres=filter_thres)\n",
    "\n",
    "            # lift_probs = F.softmax(filtered_lift_logits / temperature, dim=-1)\n",
    "            pitch_probs = F.softmax(filtered_pitch_logits / temperature, dim=-1)\n",
    "            rhythm_probs = F.softmax(filtered_rhythm_logits / temperature, dim=-1)\n",
    "\n",
    "            # lift_sample = torch.multinomial(lift_probs, 1)\n",
    "            pitch_sample = torch.multinomial(pitch_probs, 1)\n",
    "            rhythm_sample = torch.multinomial(rhythm_probs, 1)\n",
    "\n",
    "            # out_lift = torch.cat((out_lift, lift_sample), dim=-1)\n",
    "            out_pitch = torch.cat((out_pitch, pitch_sample), dim=-1)\n",
    "            out_rhythm = torch.cat((out_rhythm, rhythm_sample), dim=-1)\n",
    "            mask = F.pad(mask, (0, 1), value=True)\n",
    "\n",
    "            if (\n",
    "                eos_token is not None\n",
    "                and (torch.cumsum(out_rhythm == eos_token, 1)[:, -1] >= 1).all()\n",
    "            ):\n",
    "                break\n",
    "\n",
    "        # out_lift = out_lift[:, t:]\n",
    "        out_pitch = out_pitch[:, t:]\n",
    "        out_rhythm = out_rhythm[:, t:]\n",
    "\n",
    "        if num_dims == 1:\n",
    "            out = out.squeeze(0)\n",
    "\n",
    "        self.net.train(was_training)\n",
    "        return out_rhythm, out_pitch\n",
    "\n",
    "    def forward(self, rhythms, pitchs, notes, **kwargs):\n",
    "        # liftsi = lifts[:, :-1]\n",
    "        # liftso = lifts[:, 1:]\n",
    "        pitchsi = pitchs[:, :-1]\n",
    "        pitchso = pitchs[:, 1:]\n",
    "        rhythmsi = rhythms[:, :-1]\n",
    "        rhythmso = rhythms[:, 1:]\n",
    "        noteso = notes[:, 1:]\n",
    "\n",
    "        mask = kwargs.get(\"mask\", None)\n",
    "        if mask is not None and mask.shape[1] == rhythms.shape[1]:\n",
    "            mask = mask[:, :-1]\n",
    "            kwargs[\"mask\"] = mask\n",
    "\n",
    "        rhythmsp, pitchsp, notesp, x = self.net(rhythmsi, pitchsi, **kwargs)\n",
    "\n",
    "        loss_consist = self.calConsistencyLoss(rhythmsp, pitchsp, notesp)\n",
    "        loss_rhythm = F.cross_entropy(\n",
    "            rhythmsp.transpose(1, 2), rhythmso, ignore_index=self.ignore_index\n",
    "        )\n",
    "        loss_pitch = F.cross_entropy(\n",
    "            pitchsp.transpose(1, 2), pitchso, ignore_index=self.ignore_index\n",
    "        )\n",
    "        # loss_lift = F.cross_entropy(\n",
    "        #     liftsp.transpose(1, 2), liftso, ignore_index=self.ignore_index\n",
    "        # )\n",
    "        loss_note = F.cross_entropy(\n",
    "            notesp.transpose(1, 2), noteso, ignore_index=self.ignore_index\n",
    "        )\n",
    "\n",
    "        return dict(\n",
    "            loss_rhythm=loss_rhythm,\n",
    "            loss_pitch=loss_pitch,\n",
    "            # loss_lift=loss_lift,\n",
    "            loss_consist=loss_consist,\n",
    "            loss_note=loss_note,\n",
    "        )\n",
    "\n",
    "    def calConsistencyLoss(self, rhythmsp, pitchsp, notesp, gamma=10):\n",
    "        notesp_soft = torch.softmax(notesp, dim=2)\n",
    "        note_flag = notesp_soft[:, :, 1]\n",
    "        rhythmsp_soft = torch.softmax(rhythmsp, dim=2)\n",
    "        rhythmsp_note = torch.sum(rhythmsp_soft * self.note_mask, dim=2)\n",
    "\n",
    "        pitchsp_soft = torch.softmax(pitchsp, dim=2)\n",
    "        pitchsp_note = torch.sum(pitchsp_soft[:, :, 1:], dim=2)\n",
    "\n",
    "        # liftsp_soft = torch.softmax(liftsp, dim=2)\n",
    "        # liftsp_note = torch.sum(liftsp_soft[:, :, 1:], dim=2)\n",
    "\n",
    "        loss = (\n",
    "            gamma\n",
    "            * (\n",
    "                F.l1_loss(rhythmsp_note, note_flag)\n",
    "                # + F.l1_loss(note_flag, liftsp_note)\n",
    "                + F.l1_loss(note_flag, pitchsp_note)\n",
    "            )\n",
    "            / 3.0\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "\n",
    "def get_decoder(args):\n",
    "    return ScoreDecoder(\n",
    "        ScoreTransformerWrapper(\n",
    "            num_note_tokens=args.num_note_tokens,\n",
    "            num_rhythm_tokens=args.num_rhythm_tokens,\n",
    "            num_pitch_tokens=args.num_pitch_tokens,\n",
    "            # num_lift_tokens=args.num_lift_tokens,\n",
    "            max_seq_len=args.max_seq_len,\n",
    "            emb_dim=args.decoder_dim,\n",
    "            attn_layers=Decoder(\n",
    "                dim=args.decoder_dim,\n",
    "                depth=args.decoder_depth,\n",
    "                heads=args.decoder_heads,\n",
    "                **args.decoder_args\n",
    "            ),\n",
    "        ),\n",
    "        pad_value=args.pad_token,\n",
    "        num_rhythmtoken=args.num_rhythmtoken,\n",
    "        noteindexes=args.noteindexes,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TrOMR(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.encoder = get_encoder(args)\n",
    "        self.decoder = get_decoder(args)\n",
    "        self.args = args\n",
    "\n",
    "    def forward(self, inputs, rhythms_seq, pitchs_seq, note_seq, mask, **kwargs):\n",
    "        encoded = self.encoder(inputs)\n",
    "        loss = self.decoder(\n",
    "            rhythms_seq, pitchs_seq, note_seq, context=encoded, mask=mask, **kwargs\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, x: torch.Tensor, temperature: float = 0.25):\n",
    "        start_token = (torch.LongTensor([self.args.bos_token] * len(x))[:, None]).to(\n",
    "            x.device\n",
    "        )\n",
    "        nonote_token = (\n",
    "            torch.LongTensor([self.args.nonote_token] * len(x))[:, None]\n",
    "        ).to(x.device)\n",
    "\n",
    "        out_pitch, out_rhythm = self.decoder.generate(\n",
    "            start_token,\n",
    "            nonote_token,\n",
    "            self.args.max_seq_len,\n",
    "            eos_token=self.args.eos_token,\n",
    "            context=self.encoder(x),\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "        return out_pitch, out_rhythm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import albumentations as alb\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "# from model import TrOMR\n",
    "\n",
    "\n",
    "class StaffToScore(object):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.size_h = args.max_height\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = TrOMR(args)\n",
    "        # self.model.load_state_dict(torch.load(args.filepaths.checkpoint), strict=True)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # self.lifttokenizer = PreTrainedTokenizerFast(\n",
    "        #     tokenizer_file=args.filepaths.lifttokenizer\n",
    "        # )\n",
    "        self.pitchtokenizer = PreTrainedTokenizerFast(\n",
    "            tokenizer_file=args.filepaths.pitchtokenizer\n",
    "        )\n",
    "        self.rhythmtokenizer = PreTrainedTokenizerFast(\n",
    "            tokenizer_file=args.filepaths.rhythmtokenizer\n",
    "        )\n",
    "        self.transform = alb.Compose(\n",
    "            [\n",
    "                alb.ToGray(always_apply=True),\n",
    "                alb.Normalize((0.7931, 0.7931, 0.7931), (0.1738, 0.1738, 0.1738)),\n",
    "                ToTensorV2(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def readimg(self, path):\n",
    "        # img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "        img = cv2.imread(path)\n",
    "        print(f\"1 -- resize 전\")\n",
    "        print(img.shape)\n",
    "\n",
    "        if img.shape[-1] == 4:\n",
    "            img = 255 - img[:, :, 3]\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "        elif img.shape[-1] == 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        else:\n",
    "            raise RuntimeError(\"Unsupport image type!\")\n",
    "\n",
    "        h, w, c = img.shape\n",
    "        new_h = self.size_h\n",
    "        new_w = int(self.size_h / h * w)\n",
    "        new_w = new_w // self.args.patch_size * self.args.patch_size\n",
    "        img = cv2.resize(img, (new_w, new_h))\n",
    "        img = self.transform(image=img)[\"image\"][:1]\n",
    "\n",
    "        print(f\"2 -- resize 후\")\n",
    "        print(img.shape)\n",
    "        print(img.dtype)\n",
    "        return img\n",
    "\n",
    "    def preprocessing(self, rgb):\n",
    "        patches = rearrange(\n",
    "            rgb,\n",
    "            \"b c (h s1) (w s2) -> b (h w) (s1 s2 c)\",\n",
    "            s1=self.args.patch_size,\n",
    "            s2=self.args.patch_size,\n",
    "        )\n",
    "        return patches\n",
    "\n",
    "    def train_img2token(self, x, y):\n",
    "        if not isinstance(x, list):\n",
    "            x = [x]\n",
    "        imgs = [self.preprocessing(item) for item in x]\n",
    "        imgs = torch.cat(imgs).float().unsqueeze(1)\n",
    "\n",
    "\n",
    "    def detokenize(self, tokens, tokenizer):\n",
    "        toks = [tokenizer.convert_ids_to_tokens(tok) for tok in tokens]\n",
    "        for b in range(len(toks)):\n",
    "            for i in reversed(range(len(toks[b]))):\n",
    "                if toks[b][i] is None:\n",
    "                    toks[b][i] = \"\"\n",
    "                toks[b][i] = toks[b][i].replace(\"Ġ\", \" \").strip()\n",
    "                if toks[b][i] in ([\"[BOS]\", \"[EOS]\", \"[PAD]\"]):\n",
    "                    del toks[b][i]\n",
    "        return toks\n",
    "    def entokenize(self, tokens, tokenizer):\n",
    "        toks = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tok)) for tok in tokens]\n",
    "        return toks\n",
    "\n",
    "    def all_entokenize(self, rhythm_y_list, pitch_y_list):\n",
    "        token_rhythm = self.entokenize(rhythm_y_list, self.rhythmtokenizer)\n",
    "        token_pitch = self.entokenize(pitch_y_list, self.pitchtokenizer)\n",
    "        return token_rhythm, token_pitch\n",
    "    \n",
    "\n",
    "    def train_model(self, inputs, rhythms_seq, pitchs_seq):\n",
    "        input = inputs.to(self.device)\n",
    "        \n",
    "        return self.model.forward(input, rhythms_seq, pitchs_seq, [], [])\n",
    "\n",
    "\n",
    "    # def predict_img2token(self, rgbimgs):\n",
    "    #     if not isinstance(rgbimgs, list):\n",
    "    #         rgbimgs = [rgbimgs]\n",
    "    #     imgs = [self.preprocessing(item) for item in rgbimgs]\n",
    "    #     imgs = torch.cat(imgs).float().unsqueeze(1)\n",
    "    #     output = self.model.generate(\n",
    "    #         imgs.to(self.device), temperature=self.args.get(\"temperature\", 0.2)\n",
    "    #     )\n",
    "    #     rhythm, pitch, lift = output\n",
    "    #     return rhythm, pitch, lift\n",
    "\n",
    "    # def predict_token(self, imgpath):\n",
    "    #     imgs = []\n",
    "    #     if os.path.isdir(imgpath):\n",
    "    #         for item in os.listdir(imgpath):\n",
    "    #             imgs.append(self.readimg(os.path.join(imgpath, item)))\n",
    "    #     else:\n",
    "    #         imgs.append(self.readimg(imgpath))\n",
    "    #     imgs = torch.cat(imgs).float().unsqueeze(1)\n",
    "    #     output = self.model.generate(\n",
    "    #         imgs.to(self.device), temperature=self.args.get(\"temperature\", 0.2)\n",
    "    #     )\n",
    "    #     rhythm, pitch, lift = output\n",
    "    #     return rhythm, pitch, lift\n",
    "\n",
    "    # def predict(self, imgpath):\n",
    "    #     rhythm, pitch, lift = self.predict_token(imgpath)\n",
    "\n",
    "    #     predlift = self.detokenize(lift, self.lifttokenizer)\n",
    "    #     predpitch = self.detokenize(pitch, self.pitchtokenizer)\n",
    "    #     predrhythm = self.detokenize(rhythm, self.rhythmtokenizer)\n",
    "    #     return predrhythm, predpitch, predlift\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "def getconfig(configpath):\n",
    "    args = OmegaConf.load(configpath)\n",
    "\n",
    "    workspace = os.path.dirname(configpath)\n",
    "    for key in args.filepaths.keys():\n",
    "        args.filepaths[key] = os.path.join(workspace, args.filepaths[key])\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -- resize 전\n",
      "(120, 1000, 3)\n",
      "2 -- resize 후\n",
      "torch.Size([1, 128, 1056])\n",
      "torch.float32\n",
      "1:../data/processed-feature/transformer/Rock-ver/stave/Rock-ver_pad-stave-origin_10_2024-05-04_16-30-26.png\n",
      " -- rgbimgs : torch.Size([1, 128, 1056])\n",
      "1 -- resize 전\n",
      "(120, 1000, 3)\n",
      "2 -- resize 후\n",
      "torch.Size([1, 128, 1056])\n",
      "torch.float32\n",
      "2:../data/processed-feature/transformer/Rock-ver/stave/Rock-ver_pad-stave-origin_1_2024-05-04_16-30-26.png\n",
      " -- rgbimgs : torch.Size([1, 128, 1056])\n",
      "1 -- resize 전\n",
      "(120, 1000, 3)\n",
      "2 -- resize 후\n",
      "torch.Size([1, 128, 1056])\n",
      "torch.float32\n",
      "3:../data/processed-feature/transformer/Rock-ver/stave/Rock-ver_pad-stave-origin_2_2024-05-04_16-30-26.png\n",
      " -- rgbimgs : torch.Size([1, 128, 1056])\n",
      "1 -- resize 전\n",
      "(120, 1000, 3)\n",
      "2 -- resize 후\n",
      "torch.Size([1, 128, 1056])\n",
      "torch.float32\n",
      "4:../data/processed-feature/transformer/Rock-ver/stave/Rock-ver_pad-stave-origin_3_2024-05-04_16-30-26.png\n",
      " -- rgbimgs : torch.Size([1, 128, 1056])\n",
      "1 -- resize 전\n",
      "(120, 1000, 3)\n",
      "2 -- resize 후\n",
      "torch.Size([1, 128, 1056])\n",
      "torch.float32\n",
      "5:../data/processed-feature/transformer/Rock-ver/stave/Rock-ver_pad-stave-origin_4_2024-05-04_16-30-26.png\n",
      " -- rgbimgs : torch.Size([1, 128, 1056])\n",
      "1 -- resize 전\n",
      "(120, 1000, 3)\n",
      "2 -- resize 후\n",
      "torch.Size([1, 128, 1056])\n",
      "torch.float32\n",
      "6:../data/processed-feature/transformer/Rock-ver/stave/Rock-ver_pad-stave-origin_5_2024-05-04_16-30-26.png\n",
      " -- rgbimgs : torch.Size([1, 128, 1056])\n",
      "1 -- resize 전\n",
      "(120, 1000, 3)\n",
      "2 -- resize 후\n",
      "torch.Size([1, 128, 1056])\n",
      "torch.float32\n",
      "7:../data/processed-feature/transformer/Rock-ver/stave/Rock-ver_pad-stave-origin_6_2024-05-04_16-30-26.png\n",
      " -- rgbimgs : torch.Size([1, 128, 1056])\n",
      "1 -- resize 전\n",
      "(120, 1000, 3)\n",
      "2 -- resize 후\n",
      "torch.Size([1, 128, 1056])\n",
      "torch.float32\n",
      "8:../data/processed-feature/transformer/Rock-ver/stave/Rock-ver_pad-stave-origin_7_2024-05-04_16-30-26.png\n",
      " -- rgbimgs : torch.Size([1, 128, 1056])\n",
      "1 -- resize 전\n",
      "(120, 1000, 3)\n",
      "2 -- resize 후\n",
      "torch.Size([1, 128, 1056])\n",
      "torch.float32\n",
      "9:../data/processed-feature/transformer/Rock-ver/stave/Rock-ver_pad-stave-origin_8_2024-05-04_16-30-26.png\n",
      " -- rgbimgs : torch.Size([1, 128, 1056])\n",
      "1 -- resize 전\n",
      "(120, 1000, 3)\n",
      "2 -- resize 후\n",
      "torch.Size([1, 128, 1056])\n",
      "torch.float32\n",
      "10:../data/processed-feature/transformer/Rock-ver/stave/Rock-ver_pad-stave-origin_9_2024-05-04_16-30-26.png\n",
      " -- rgbimgs : torch.Size([1, 128, 1056])\n",
      "------------------y data----------------------------\n",
      "-- labeling y : 10\n",
      "----------------------------------------------------\n",
      "[BOS]+clef-percussion+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+barline+rest-eighth+rest-eighth+note-eighth+note-eighth+note-eighth+note-eighth+note-eighth+note-eighth+barline+note-quarter|note-quarter+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-quarter|note-quarter+barline+note-quarter|note-quarter+rest-quarter+rest-quarter+barline+rest-quarter+barline+rest-quarter+barline+rest-quarter+barline+rest-quarter+barline+[EOS]\n",
      "[BOS]+clef-percussion+timeSignature-4/4+note-quarter+note-quarter+note-quarter+note-quarter+note-quarter+barline+note-quarter|note-quarter+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth+barline+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth+barline+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+barline+[EOS]\n",
      "[BOS]+clef-percussion+rest-eighth+note-eighth+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-quarter|note-quarter+barline+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth+barline+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth+barline+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+barline+[EOS]\n",
      "[BOS]+clef-percussion+rest-eighth+rest-eighth+rest-eighth+note-eighth+note-eighth+note-eighth+barline+note-quarter|note-quarter+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth+barline+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth+barline+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth+barline+[EOS]\n",
      "[BOS]+clef-percussion+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+rest-eighth+barline+note-quarter|note-quarter+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth+barline+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth+barline+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth+barline+[EOS]\n",
      "[BOS]+clef-percussion+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth+note-eighth+note-eighth|note-eighth+rest-eighth+barline+note-quarter|note-quarter+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+barline+rest-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+rest-eighth+note-eighth+note-eighth|note-eighth+note-eighth+barline+note-quarter|note-quarter+note-quarter|note-quarter+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+barline+[EOS]\n",
      "[BOS]+clef-percussion+rest-eighth+note-eighth+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth+barline+note-quarter|note-quarter+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+barline+rest-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+rest-eighth+note-eighth+note-eighth|note-eighth+note-eighth+barline+note-quarter|note-quarter+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+barline+[EOS]\n",
      "[BOS]+clef-percussion+rest-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+barline+rest-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth+note-eighth+note-eighth+barline+note-quarter|note-quarter+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth+barline+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+barline+[EOS]\n",
      "[BOS]+clef-percussion+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth+barline+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth+barline+note-quarter|note-quarter+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth+barline+note-quarter|note-quarter+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth+barline+[EOS]\n",
      "[BOS]+clef-percussion+note-quarter|note-quarter+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth+barline+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth+note-eighth+note-eighth+barline+note-quarter|note-quarter+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth+barline+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth|note-eighth+note-eighth+barline+[EOS]\n",
      "------------------processing y data----------------------------\n",
      "-- token_rhythm : 10\n",
      "-- token_rhythm : [[1, 6, 7, 4, 7, 7, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 5, 19, 19, 7, 7, 7, 7, 7, 7, 5, 11, 4, 11, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 11, 4, 11, 5, 11, 4, 11, 23, 23, 5, 23, 5, 23, 5, 23, 5, 23, 5, 2], [1, 6, 31, 11, 11, 11, 11, 11, 5, 11, 4, 11, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 5, 7, 4, 7, 7, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 5, 7, 4, 7, 7, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 5, 2], [1, 6, 19, 7, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 11, 4, 11, 5, 7, 4, 7, 7, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 5, 7, 4, 7, 7, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 5, 7, 4, 7, 7, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 5, 2], [1, 6, 19, 19, 19, 7, 7, 7, 5, 11, 4, 11, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 5, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 5, 7, 4, 7, 7, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 5, 2], [1, 6, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 19, 5, 11, 4, 11, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 5, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 5, 7, 4, 7, 7, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 5, 2], [1, 6, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 7, 7, 4, 7, 19, 5, 11, 4, 11, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 5, 19, 7, 7, 4, 7, 7, 4, 7, 19, 7, 7, 4, 7, 7, 5, 11, 4, 11, 11, 4, 11, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 5, 2], [1, 6, 19, 7, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 5, 11, 4, 11, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 5, 19, 7, 7, 4, 7, 7, 4, 7, 19, 7, 7, 4, 7, 7, 5, 11, 4, 11, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 5, 2], [1, 6, 19, 7, 7, 4, 7, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 5, 19, 7, 7, 4, 7, 7, 4, 7, 7, 7, 4, 7, 7, 7, 7, 5, 11, 4, 11, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 5, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 5, 2], [1, 6, 7, 4, 7, 7, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 5, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 5, 11, 4, 11, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 5, 11, 4, 11, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 5, 2], [1, 6, 11, 4, 11, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 5, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 7, 7, 5, 11, 4, 11, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 5, 7, 4, 7, 7, 7, 4, 7, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 5, 2]]\n",
      "-- token_pitch : 10\n",
      "-- token_pitch : [[0, 3, 11, 11, 7, 11, 11, 3, 11, 3, 11, 7, 11, 3, 12, 0, 0, 0, 9, 9, 8, 3, 7, 7, 0, 3, 12, 7, 11, 3, 11, 7, 11, 3, 11, 5, 8, 0, 3, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 11, 11, 11, 7, 7, 0, 3, 12, 7, 11, 11, 3, 11, 3, 11, 7, 11, 11, 0, 3, 11, 11, 7, 11, 11, 3, 11, 3, 11, 7, 11, 11, 0, 3, 11, 11, 7, 11, 11, 3, 11, 3, 11, 7, 11, 3, 12, 0], [0, 0, 3, 7, 11, 11, 3, 11, 3, 11, 7, 12, 0, 3, 12, 11, 7, 11, 11, 3, 11, 3, 11, 7, 11, 11, 0, 3, 11, 11, 7, 11, 11, 3, 11, 3, 11, 7, 11, 11, 0, 3, 11, 11, 7, 11, 11, 3, 11, 3, 11, 7, 11, 3, 12, 0], [0, 0, 0, 0, 3, 7, 7, 0, 3, 12, 7, 11, 11, 3, 11, 3, 11, 7, 11, 11, 0, 3, 11, 11, 7, 11, 3, 11, 11, 3, 11, 7, 11, 11, 0, 3, 11, 11, 7, 11, 11, 3, 11, 3, 11, 7, 11, 11, 0], [0, 3, 11, 11, 7, 11, 3, 11, 7, 11, 3, 12, 0, 0, 3, 12, 7, 11, 11, 3, 11, 3, 11, 7, 11, 11, 0, 3, 11, 11, 7, 11, 3, 11, 11, 3, 11, 7, 11, 11, 0, 3, 11, 11, 7, 11, 11, 3, 11, 3, 11, 7, 11, 11, 0], [0, 3, 11, 11, 7, 11, 3, 11, 7, 7, 3, 12, 0, 0, 3, 12, 7, 11, 11, 3, 11, 3, 11, 7, 11, 3, 12, 0, 0, 3, 7, 11, 3, 12, 0, 3, 7, 11, 11, 0, 3, 12, 7, 12, 3, 11, 3, 11, 7, 11, 3, 12, 0], [0, 0, 3, 7, 11, 11, 3, 11, 3, 11, 7, 11, 11, 0, 3, 12, 7, 11, 11, 3, 11, 3, 11, 7, 11, 3, 12, 0, 0, 3, 7, 11, 3, 12, 0, 3, 7, 11, 11, 0, 3, 12, 7, 11, 11, 3, 11, 3, 11, 7, 11, 3, 12, 0], [0, 0, 3, 7, 11, 3, 11, 11, 3, 11, 7, 11, 3, 12, 0, 0, 3, 7, 11, 3, 11, 11, 3, 11, 9, 9, 8, 0, 3, 12, 7, 11, 11, 3, 11, 3, 11, 7, 11, 11, 0, 3, 11, 3, 11, 7, 11, 3, 11, 11, 3, 11, 7, 11, 3, 11, 0], [0, 3, 11, 11, 7, 11, 11, 3, 11, 3, 11, 7, 11, 11, 0, 3, 11, 3, 11, 7, 11, 3, 11, 11, 3, 11, 7, 11, 11, 0, 3, 12, 7, 11, 11, 3, 11, 3, 11, 7, 11, 11, 0, 3, 12, 7, 11, 11, 3, 11, 3, 11, 7, 11, 11, 0], [0, 3, 12, 7, 11, 11, 3, 11, 3, 11, 7, 11, 11, 0, 3, 11, 11, 7, 11, 3, 11, 7, 11, 3, 11, 7, 9, 8, 0, 3, 12, 7, 11, 11, 3, 11, 3, 11, 7, 11, 11, 0, 3, 11, 11, 7, 11, 11, 3, 11, 3, 11, 7, 11, 11, 0]]\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Module.train() takes from 1 to 2 positional arguments but 6 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 252\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-- token_pitch : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken_pitch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 252\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mhandler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_rhythm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_pitch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 106\u001b[0m, in \u001b[0;36mStaffToScore.train_model\u001b[0;34m(self, inputs, rhythms_seq, pitchs_seq)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, rhythms_seq, pitchs_seq):\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhythms_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpitchs_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Module.train() takes from 1 to 2 positional arguments but 6 were given"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "from random import randrange\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --\n",
    "    # parser = argparse.ArgumentParser(description=\"Inference single staff image\")\n",
    "    # parser.add_argument(\"filepath\", type=str, help=\"path to staff image\")\n",
    "    # parsed_args = parser.parse_args()\n",
    "\n",
    "    # os.path.dirname(__file__): 현재 실행 중인 스크립트의 파일 경로\n",
    "    cofigpath = \"../src/workspace/config.yaml\" \n",
    "    args = getconfig(cofigpath)\n",
    "\n",
    "    handler = StaffToScore(args)\n",
    "\n",
    "    x_dataset_path=f\"{DATA_FEATURE_PATH}/transformer/Rock-ver/stave/\"\n",
    "    x_all_dataset_path = glob.glob(f\"{x_dataset_path}/*\")\n",
    "    x_file_list = [file for file in x_all_dataset_path if file.endswith(f\".png\")]\n",
    "    x_file_list.sort()\n",
    "\n",
    "    # imgpath = f\"{DATA_TEST_PATH}/test-01.png\"\n",
    "\n",
    "    y_dataset_path=f\"{DATA_FEATURE_PATH}/transformer/Rock-ver/annotation/\"\n",
    "    y_all_dataset_path = glob.glob(f\"{y_dataset_path}/*\")\n",
    "    y_file_list = [file for file in y_all_dataset_path if file.endswith(f\".txt\")]\n",
    "    y_file_list.sort()\n",
    "    \n",
    "\n",
    "    def convert_img(imgpath):\n",
    "        imgs = []\n",
    "        if os.path.isdir(imgpath):\n",
    "            for item in os.listdir(imgpath):\n",
    "                imgs.append(handler.readimg(os.path.join(imgpath, item)))\n",
    "        else:\n",
    "            imgs.append(handler.readimg(imgpath))\n",
    "        imgs = torch.cat(imgs).float()\n",
    "\n",
    "        return imgs\n",
    "\n",
    "    \"\"\"\n",
    "    1 -- resize 전\n",
    "    (298, 2404, 4)\n",
    "    2 -- resize 후\n",
    "    torch.Size([1, 128, 1024])\n",
    "    torch.float32\n",
    "    rgbimgs : torch.Size([1, 1, 128, 1024])\n",
    "    \"\"\"\n",
    "\n",
    "    x_list=[]\n",
    "    for idx, x_file in enumerate(x_file_list):\n",
    "        convert_x_file = convert_img(x_file)\n",
    "        print(f\"{idx+1}:{x_file}\\n -- rgbimgs : {convert_x_file.shape}\")\n",
    "        x_list.append(convert_x_file)\n",
    "\n",
    "    concatenated_x_list = torch.cat(x_list, dim=0)\n",
    "\n",
    "    # 얘가 encoder input임!!!!!!!!!! concatenated_x_list.unsqueeze(1)\n",
    "    inputs = concatenated_x_list.unsqueeze(1)\n",
    "\n",
    "    # imgs = handler.preprocessing(concatenated_x_list.unsqueeze(1))\n",
    "    # print(f\"------------------preprocessing---------------------\")\n",
    "    # print(f\"-- preprocessing x : {imgs.shape}\")\n",
    "    # print(f\"----------------------------------------------------\")\n",
    "\n",
    "\n",
    "    def read_txt_file(file_path):\n",
    "        \"\"\"\n",
    "        텍스트 파일을 읽어서 내용을 리스트로 반환하는 함수\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.readlines()\n",
    "            # 각 줄의 개행 문자 제거\n",
    "            content = [line.strip() for line in content]\n",
    "        return content[0]\n",
    "    # 각 파일의 내용을 담을 리스트\n",
    "    contents = []\n",
    "\n",
    "    # 각 파일을 읽어서 내용을 리스트에 추가\n",
    "    for txt_path in y_file_list:\n",
    "        # print(\"--- txt_path:\", txt_path)\n",
    "        content = read_txt_file(txt_path)\n",
    "        # print(content)\n",
    "        contents.append(content)\n",
    "\n",
    "    print(f\"------------------y data----------------------------\")\n",
    "    print(f\"-- labeling y : {len(contents)}\")\n",
    "    print(f\"----------------------------------------------------\")\n",
    "\n",
    "    # 각 token에 맞는 string list로 만들기\n",
    "\n",
    "    def map_pitch(note):\n",
    "        pitch_mapping = {\n",
    "            \"note-D4\": 1,\n",
    "            \"note-E4\": 2,\n",
    "            \"note-F4\": 3,\n",
    "            \"note-G4\": 4,\n",
    "            \"note-A4\": 5,\n",
    "            \"note-B4\": 6,\n",
    "            \"note-C5\": 7,\n",
    "            \"note-D5\": 8,\n",
    "            \"note-E5\": 9,\n",
    "            \"note-F5\": 10,\n",
    "            \"note-G5\": 11,\n",
    "            \"note-A5\": 12,\n",
    "            \"note-B5\": 13\n",
    "        }\n",
    "        return \"nonote\" if note not in pitch_mapping else note\n",
    "    \n",
    "    def map_duration(note):\n",
    "        duration_mapping =  {\n",
    "            \"[PAD]\": 0,\n",
    "            \"[BOS]\": 1,\n",
    "            \"[EOS]\": 2,\n",
    "            \"+\": 3,\n",
    "            \"|\": 4,\n",
    "            \"barline\": 5,\n",
    "            \"clef-percussion\":6,\n",
    "\n",
    "            \"note-eighth\": 7,\n",
    "            \"note-eighth.\": 8,\n",
    "            \"note-half\": 9,\n",
    "            \"note-half.\": 10,\n",
    "            \"note-quarter\": 11,\n",
    "            \"note-quarter.\": 12,\n",
    "            \"note-sixteenth\": 13,\n",
    "            \"note-sixteenth.\": 14,\n",
    "            \"note-thirty_second\": 15,\n",
    "            \"note-thirty_second.\": 16,\n",
    "            \"note-whole\": 17,\n",
    "            \"note-whole.\": 18,\n",
    "\n",
    "            \"rest-eighth\": 19,\n",
    "            \"rest-eighth.\": 20,\n",
    "            \"rest-half\": 21,\n",
    "            \"rest-half.\": 22,\n",
    "            \"rest-quarter\": 23,\n",
    "            \"rest-quarter.\": 24,\n",
    "            \"rest-sixteenth\": 25,\n",
    "            \"rest-sixteenth.\": 26,\n",
    "            \"rest-thirty_second\": 27,\n",
    "            \"rest-thirty_second.\": 28,\n",
    "            \"rest-whole\": 29,\n",
    "            \"rest-whole.\": 30,\n",
    "\n",
    "            \"timeSignature-4/4\": 31,\n",
    "        }\n",
    "        return note if note in duration_mapping else \"\"\n",
    "\n",
    "    def map_notes2pitch(note_list):\n",
    "        result=[]\n",
    "        for notes in note_list:\n",
    "            # print(notes)\n",
    "            group_notes = []\n",
    "            # 우선 +로 나누고, 안에 | 있는 지 확인해서 먼저 붙이기\n",
    "            note_split = notes.split(\"+\")\n",
    "            # print(note_split)\n",
    "            i_idx=0\n",
    "            while i_idx<len(note_split):\n",
    "                note_s = note_split[i_idx]\n",
    "                if \"|\" in note_s:\n",
    "                    mapped_note_chord = []\n",
    "                    note_split_chord = note_s.split(\"|\")\n",
    "                    \n",
    "                    # | 로 나눈 만큼 건너뛰기\n",
    "                    for note_s_c in note_split_chord:\n",
    "                        note_pitch, _ = note_s_c.split(\"_\")\n",
    "                        mapped_note_chord.append(f\"{map_pitch(note_pitch)}\")\n",
    "                    group_notes.append(\"+\".join(mapped_note_chord))\n",
    "                    i_idx+=1\n",
    "                elif \"note\" in note_s:\n",
    "                    if \"_\" in note_s:\n",
    "                        note_pitch, _ = note_s.split(\"_\")\n",
    "                        group_notes.append(f\"{map_pitch(note_pitch)}\")\n",
    "                        i_idx+=1\n",
    "                else:\n",
    "                    group_notes.append(f\"{map_pitch(note_s)}\")\n",
    "                    i_idx+=1\n",
    "            # print(group_notes)\n",
    "            result.append(\"+\".join(group_notes))\n",
    "        return result\n",
    "    \n",
    "    def map_notes2rhythm(note_list):\n",
    "        result=[]\n",
    "        for notes in note_list:\n",
    "            group_notes = [\"[BOS]\"]\n",
    "            # 우선 +로 나누고, 안에 | 있는 지 확인해서 먼저 붙이기\n",
    "            note_split = notes.split(\"+\")\n",
    "            i_idx=0\n",
    "            while i_idx<len(note_split):\n",
    "                note_s = note_split[i_idx]\n",
    "                if \"|\" in note_s:\n",
    "                    mapped_note_chord = []\n",
    "                    note_split_chord = note_s.split(\"|\")\n",
    "                    \n",
    "                    # | 로 나눈 만큼 건너뛰기\n",
    "                    for note_s_c in note_split_chord:\n",
    "                        # note-thirty_second 가 있기 때문에, 이걸 어떻게 잇지? _ 로 split해버리면 thirty second로 분리되자나\n",
    "                        # 분리된 거에서 첫 번째 pitch 정보는 버리고, _로 join 해놓고 \n",
    "                        note_s_c_split = note_s_c.split(\"_\")\n",
    "                        note_s_c_split = note_s_c_split[1:]\n",
    "                        note_duration = \"_\".join(note_s_c_split)\n",
    "\n",
    "                        mapped_note_chord.append(map_duration(f\"note-{note_duration}\"))\n",
    "                    group_notes.append(\"|\".join(mapped_note_chord))\n",
    "                    i_idx+=1\n",
    "                elif \"note\" in note_s:\n",
    "                    if \"_\" in note_s:\n",
    "                        note_s_c_split = note_s_c.split(\"_\")\n",
    "                        note_s_c_split = note_s_c_split[1:]\n",
    "                        note_duration = \"_\".join(note_s_c_split)\n",
    "\n",
    "                        group_notes.append(map_duration(f\"note-{note_duration}\"))\n",
    "                        i_idx+=1\n",
    "                elif \"rest\" in note_s:\n",
    "                    if \"_\" in note_s:\n",
    "                        note_s_c_split = note_s_c.split(\"_\")\n",
    "                        note_s_c_split = note_s_c_split[1:]\n",
    "                        note_duration = \"_\".join(note_s_c_split)\n",
    "\n",
    "                        group_notes.append(map_duration(f\"rest-{note_duration}\"))\n",
    "                        i_idx+=1\n",
    "                else:\n",
    "                    group_notes.append(map_duration(note_s))\n",
    "                    i_idx+=1\n",
    "            # print(group_notes)\n",
    "            group_notes.append(\"[EOS]\")\n",
    "            result.append(\"+\".join(group_notes))\n",
    "            print(\"+\".join(group_notes))\n",
    "\n",
    "        return result\n",
    "\n",
    "    rhythm_contents = map_notes2rhythm(contents)\n",
    "    pitch_contents = map_notes2pitch(contents)\n",
    "    token_rhythm, token_pitch = handler.all_entokenize(rhythm_contents, pitch_contents)\n",
    "    print(f\"------------------processing y data----------------------------\")\n",
    "    print(f\"-- token_rhythm : {len(token_rhythm)}\")\n",
    "    print(f\"-- token_rhythm : {token_rhythm}\")\n",
    "    print(f\"-- token_pitch : {len(token_pitch)}\")\n",
    "    print(f\"-- token_pitch : {token_pitch}\")\n",
    "    print(f\"----------------------------------------------------\")\n",
    "\n",
    "\n",
    "    model = handler.train_model(inputs, token_rhythm, token_pitch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
